{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "b18f6db9-2313-4d53-98c2-fda8232d3a1d",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-01-19T01:56:35.979347Z",
     "iopub.status.busy": "2024-01-19T01:56:35.978913Z",
     "iopub.status.idle": "2024-01-19T01:56:39.129953Z",
     "shell.execute_reply": "2024-01-19T01:56:39.128956Z",
     "shell.execute_reply.started": "2024-01-19T01:56:35.979318Z"
    },
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[32m[01-19 09:56:39 MainThread @utils.py:73]\u001b[0m paddlepaddle version: 2.3.2.\r\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/envs/python35-paddle120-env/lib/python3.7/site-packages/parl/remote/communication.py:38: FutureWarning: 'pyarrow.default_serialization_context' is deprecated as of 2.0.0 and will be removed in a future version. Use pickle or the pyarrow IPC functionality instead.\r\n",
      "  context = pyarrow.default_serialization_context()\r\n"
     ]
    }
   ],
   "source": [
    "import argparse\r\n",
    "import os\r\n",
    "import gym\r\n",
    "import random\r\n",
    "from gym import spaces\r\n",
    "from sklearn import preprocessing\r\n",
    "import numpy as np\r\n",
    "import pandas as pd\r\n",
    "from parl.utils import logger, tensorboard, ReplayMemory\r\n",
    "import paddle\r\n",
    "from OAC import OAC\r\n",
    "import parl\r\n",
    "import paddle.nn as nn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "8acbda54-2093-4590-b38a-9988c000ac66",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-01-19T01:56:41.761480Z",
     "iopub.status.busy": "2024-01-19T01:56:41.760614Z",
     "iopub.status.idle": "2024-01-19T01:56:41.790251Z",
     "shell.execute_reply": "2024-01-19T01:56:41.789522Z",
     "shell.execute_reply.started": "2024-01-19T01:56:41.761437Z"
    },
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# 默认的一些数据，用于归一化属性值\r\n",
    "MAX_ACCOUNT_BALANCE = 2147480        # 最大的账户财产\r\n",
    "MAX_NUM_SHARES = 2147480             # 最大的手数\r\n",
    "MAX_SHARE_PRICE = 5000              # 最大的单手价格\r\n",
    "MAX_VOLUME = 1e9                 # 最大的成交量\r\n",
    "MAX_AMOUNT = 1e10                    # 最大的成交额\r\n",
    "MAX_OPEN_POSITIONS = 5              # 最大的持仓头寸\r\n",
    "MAX_STEPS = 100000                    # 最大的交互次数\r\n",
    "MAX_DAY_CHANGE = 1                  # 最大的日期改变\r\n",
    "max_loss =-300000                   # 最大的损失\r\n",
    "max_predict_rate = 1.5            # 最大的预测率\r\n",
    "INITIAL_ACCOUNT_BALANCE = 1000000    # 初始的金钱\r\n",
    "# NUM = 150 \r\n",
    "\r\n",
    "class StockTradingEnv(gym.Env):\r\n",
    "    \"\"\"A stock trading environment for OpenAI gym\"\"\"\r\n",
    "    metadata = {'render.modes': ['human']}\r\n",
    "\r\n",
    "    def __init__(self, df):\r\n",
    "        super(StockTradingEnv, self).__init__()\r\n",
    "        self.df = df\r\n",
    "        # self.reward_range = (0, MAX_ACCOUNT_BALANCE)\r\n",
    "\r\n",
    "        # 动作的可能情况：买入x%, 卖出x%, 观望\r\n",
    "        self.action_space = spaces.Box(\r\n",
    "            low=np.array([0]), high=np.array([1]), dtype=np.float32)\r\n",
    "        # self.action_space = spaces.Box(\r\n",
    "        #     low=np.array([-1,-1]), high=np.array([1,1]), dtype=np.float32)\r\n",
    "\r\n",
    "        # 环境状态的维度\r\n",
    "        self.observation_space = spaces.Box(\r\n",
    "            low=0, high=1, shape=(17,), dtype=np.float32)\r\n",
    "\r\n",
    "        self.current_step = 0\r\n",
    "        self.buy = []\r\n",
    "        self.sell = []\r\n",
    "        self.buy_v = []\r\n",
    "        self.sell_v = []\r\n",
    "        self.get = []\r\n",
    "    \r\n",
    "    def seed(self, seed):\r\n",
    "        random.seed(seed)\r\n",
    "        np.random.seed(seed)\r\n",
    "\r\n",
    "    \r\n",
    "    # 处理状态\r\n",
    "    def _next_observation(self):\r\n",
    "        obs = np.array([\r\n",
    "            self.df.loc[self.current_step, 'Open']/MAX_ACCOUNT_BALANCE,\r\n",
    "            self.df.loc[self.current_step, 'High']/MAX_ACCOUNT_BALANCE,\r\n",
    "            self.df.loc[self.current_step, 'Low']/MAX_ACCOUNT_BALANCE,\r\n",
    "            self.df.loc[self.current_step, 'Close']/MAX_ACCOUNT_BALANCE,\r\n",
    "            self.df.loc[self.current_step, 'Volume']/MAX_ACCOUNT_BALANCE,\r\n",
    "            self.df.loc[self.current_step, 'return'],\r\n",
    "            self.df.loc[self.current_step, 'rsi_rsi14']/1000,\r\n",
    "            self.df.loc[self.current_step, 'boll_upper']/MAX_ACCOUNT_BALANCE,\r\n",
    "            self.df.loc[self.current_step, 'boll_middle']/MAX_ACCOUNT_BALANCE,\r\n",
    "            self.df.loc[self.current_step, 'boll_lower']/MAX_ACCOUNT_BALANCE,\r\n",
    "            self.balance / MAX_ACCOUNT_BALANCE,\r\n",
    "            self.max_net_worth / MAX_ACCOUNT_BALANCE,\r\n",
    "            self.net_worth /  MAX_ACCOUNT_BALANCE,\r\n",
    "            self.shares_held / MAX_NUM_SHARES,\r\n",
    "            self.cost_basis / MAX_SHARE_PRICE,\r\n",
    "            self.total_shares_sold / MAX_NUM_SHARES,\r\n",
    "            self.total_sales_value / (MAX_NUM_SHARES * MAX_SHARE_PRICE)\r\n",
    "        ])\r\n",
    "        return obs\r\n",
    "\r\n",
    "\r\n",
    "    # 执行当前动作，并计算出当前的数据（如：资产等）\r\n",
    "    def _take_action(self, action):\r\n",
    "        # 随机设置当前的价格，其范围上界为当前时间点的价格\r\n",
    "        current_price =  self.df.loc[self.current_step, \"Close\"]\r\n",
    "        ris = self.df.loc[self.current_step, 'rsi_rsi14']\r\n",
    "        boll_up = self.df.loc[self.current_step, 'boll_upper']\r\n",
    "        boll_low = self.df.loc[self.current_step, 'boll_lower']\r\n",
    "        # action_type = action[0]\r\n",
    "        amount = action[0]\r\n",
    "        # current_price<boll_low and ris<30 and self.balance >= current_price\r\n",
    "        # action_type < 1/3 and self.balance >= current_price\r\n",
    "        if  current_price<=boll_low and ris<=30 and self.balance >= current_price:     # 买入amount%\r\n",
    "            self.buy.append(self.current_step)\r\n",
    "            total_possible = int(self.balance / current_price)\r\n",
    "            shares_bought = total_possible * amount\r\n",
    "            if shares_bought != 0.:\r\n",
    "                prev_cost = self.cost_basis * self.shares_held\r\n",
    "                additional_cost = shares_bought * current_price\r\n",
    "                self.buy_v.append(additional_cost)\r\n",
    "\r\n",
    "                self.balance -= additional_cost\r\n",
    "                self.cost_basis = (\r\n",
    "                    prev_cost + additional_cost) / (self.shares_held + shares_bought)\r\n",
    "                self.shares_held += shares_bought\r\n",
    "        # current_price>boll_up and ris>70 and self.shares_held != 0\r\n",
    "        # action_type > 2/3 and self.shares_held != 0\r\n",
    "        elif  current_price>=boll_up and ris>=70 and self.shares_held != 0:  # 卖出amount%\r\n",
    "            self.sell.append(self.current_step)\r\n",
    "            # print(self.states_sell)\r\n",
    "            shares_sold = self.shares_held * amount\r\n",
    "            self.balance += shares_sold * current_price\r\n",
    "            self.sell_v.append(shares_sold * current_price)\r\n",
    "            self.shares_held -= shares_sold\r\n",
    "            self.total_shares_sold += shares_sold\r\n",
    "            self.total_sales_value += shares_sold * current_price\r\n",
    "        \r\n",
    "        else:\r\n",
    "            pass\r\n",
    "\r\n",
    "        # 计算出执行动作后的资产净值\r\n",
    "        self.net_worth = self.balance + self.shares_held * current_price\r\n",
    "        self.get.append(self.net_worth)\r\n",
    "\r\n",
    "        if self.net_worth > self.max_net_worth:\r\n",
    "            self.max_net_worth = self.net_worth\r\n",
    "\r\n",
    "        if self.shares_held == 0:\r\n",
    "            self.cost_basis = 0\r\n",
    "\r\n",
    "\r\n",
    "    # 与环境交互\r\n",
    "    def step(self, action):\r\n",
    "        # 在环境内执行\r\n",
    "        self._take_action(action)\r\n",
    "        done = False\r\n",
    "        status = None\r\n",
    "\r\n",
    "        reward = 0\r\n",
    "\r\n",
    "        # 判断是否终止\r\n",
    "        self.current_step += 1\r\n",
    "\r\n",
    "        # delay_modifier = (self.current_step / MAX_STEPS)\r\n",
    "\r\n",
    "        # reward += delay_modifier\r\n",
    "\r\n",
    "        if self.net_worth >= INITIAL_ACCOUNT_BALANCE * max_predict_rate:\r\n",
    "            reward += max_predict_rate\r\n",
    "            status = f'[ENV] success at step {self.current_step}! Get {max_predict_rate} times worth.'\r\n",
    "            # self.current_step = 0\r\n",
    "            done = True\r\n",
    "        if self.current_step > len(self.df.loc[:, 'Open'].values) - 1:\r\n",
    "            status = f'[ENV] Loop training. Max worth was {self.max_net_worth}, final worth is {self.net_worth}.'\r\n",
    "            # reward += (self.net_worth / INITIAL_ACCOUNT_BALANCE - max_predict_rate) / max_predict_rate  \r\n",
    "            reward += self.net_worth / INITIAL_ACCOUNT_BALANCE\r\n",
    "            self.current_step = 0  # loop training\r\n",
    "            done = True\r\n",
    "\r\n",
    "\r\n",
    "        if self.net_worth <= 0 :\r\n",
    "            status = f'[ENV] Failure at step {self.current_step}. Loss all worth. Max worth was {self.max_net_worth}'\r\n",
    "            reward += -1\r\n",
    "            # self.current_step = 0\r\n",
    "            done = True\r\n",
    "        \r\n",
    "        else:\r\n",
    "            # 计算相对收益比，并据此来计算奖励\r\n",
    "            profit = self.net_worth - INITIAL_ACCOUNT_BALANCE\r\n",
    "            # profit = self.net_worth - self.balance\r\n",
    "            profit_percent = profit / INITIAL_ACCOUNT_BALANCE\r\n",
    "            reward += profit_percent\r\n",
    "            # if profit_percent > 0:\r\n",
    "            #     reward += profit_percent\r\n",
    "            # elif profit_percent == 0:\r\n",
    "            #     reward += -0.1\r\n",
    "            # else:\r\n",
    "            #     reward += -0.1\r\n",
    "\r\n",
    "        obs = self._next_observation()\r\n",
    "\r\n",
    "        return obs, reward, done, {\r\n",
    "            'profit': self.net_worth,\r\n",
    "            'current_step': self.current_step,\r\n",
    "            'status': status,\r\n",
    "            'buy' : self.buy,\r\n",
    "            'sell' : self.sell,\r\n",
    "            'buy_v':self.buy_v,\r\n",
    "            'sell_v':self.sell_v,\r\n",
    "            'get':self.get\r\n",
    "        }\r\n",
    "\r\n",
    "\r\n",
    "    # 重置环境\r\n",
    "    def reset(self, new_df=None):\r\n",
    "        # 重置环境的变量为初始值\r\n",
    "        self.balance = INITIAL_ACCOUNT_BALANCE\r\n",
    "        self.net_worth = INITIAL_ACCOUNT_BALANCE\r\n",
    "        self.max_net_worth = INITIAL_ACCOUNT_BALANCE\r\n",
    "        self.shares_held = 0\r\n",
    "        self.cost_basis = 0\r\n",
    "        self.total_shares_sold = 0\r\n",
    "        self.total_sales_value = 0\r\n",
    "\r\n",
    "        # 传入环境数据集\r\n",
    "        if new_df:\r\n",
    "            self.df = new_df\r\n",
    "        # if self.current_step > len(self.df.loc[:, 'open'].values) - 1:\r\n",
    "        self.current_step = 0\r\n",
    "\r\n",
    "        return self._next_observation()\r\n",
    "\r\n",
    "    \r\n",
    "    def get_obs(self, current_step):\r\n",
    "        obs = np.array([\r\n",
    "            self.df.loc[self.current_step, 'Open']/MAX_ACCOUNT_BALANCE,\r\n",
    "            self.df.loc[self.current_step, 'High']/MAX_ACCOUNT_BALANCE,\r\n",
    "            self.df.loc[self.current_step, 'Low']/MAX_ACCOUNT_BALANCE,\r\n",
    "            self.df.loc[self.current_step, 'Close']/MAX_ACCOUNT_BALANCE,\r\n",
    "            self.df.loc[self.current_step, 'Volume']/MAX_ACCOUNT_BALANCE,\r\n",
    "            self.df.loc[self.current_step, 'return'],\r\n",
    "            self.df.loc[self.current_step, 'rsi_rsi14']/1000,\r\n",
    "            self.df.loc[self.current_step, 'boll_upper']/MAX_ACCOUNT_BALANCE,\r\n",
    "            self.df.loc[self.current_step, 'boll_middle']/MAX_ACCOUNT_BALANCE,\r\n",
    "            self.df.loc[self.current_step, 'boll_lower']/MAX_ACCOUNT_BALANCE,\r\n",
    "            self.balance / MAX_ACCOUNT_BALANCE,\r\n",
    "            self.max_net_worth / MAX_ACCOUNT_BALANCE,\r\n",
    "            self.net_worth /  MAX_ACCOUNT_BALANCE,\r\n",
    "            self.shares_held / MAX_NUM_SHARES,\r\n",
    "            self.cost_basis / MAX_SHARE_PRICE,\r\n",
    "            self.total_shares_sold / MAX_NUM_SHARES,\r\n",
    "            self.total_sales_value / (MAX_NUM_SHARES * MAX_SHARE_PRICE)\r\n",
    "        ])\r\n",
    "        return obs\r\n",
    "\r\n",
    "\r\n",
    "    # 显示环境至屏幕\r\n",
    "    def render(self, mode='human'):\r\n",
    "        # 打印环境信息\r\n",
    "        # profit = self.net_worth - INITIAL_ACCOUNT_BALANCE\r\n",
    "        # print('-'*30)\r\n",
    "        # print(f'Step: {self.current_step}')\r\n",
    "        # print(f'Balance: {self.balance}')\r\n",
    "        # print(f'Shares held: {self.shares_held} (Total sold: {self.total_shares_sold})')\r\n",
    "        # print(f'Avg cost for held shares: {self.cost_basis} (Total sales value: {self.total_sales_value})')\r\n",
    "        # print(f'Net worth: {self.net_worth} (Max net worth: {self.max_net_worth})')\r\n",
    "        # print(f'Profit: {profit}')\r\n",
    "        self.sell = len(self.states_sell)\r\n",
    "        self.buy = len(self.states_buy)\r\n",
    "        print(self.buy)\r\n",
    "        print(self.sell)\r\n",
    "        return self.sell,self.buy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "f0ca2920-6cb7-40cc-8c91-487452f54f8a",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-01-19T01:56:46.395280Z",
     "iopub.status.busy": "2024-01-19T01:56:46.394722Z",
     "iopub.status.idle": "2024-01-19T01:56:46.438235Z",
     "shell.execute_reply": "2024-01-19T01:56:46.437211Z",
     "shell.execute_reply.started": "2024-01-19T01:56:46.395247Z"
    },
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                      Time      Open      High       Low     Close  \\\r\n",
      "0      2022-07-01 18:00:00  19424.45  19496.80  19344.00  19426.64   \r\n",
      "1      2022-07-01 18:30:00  19426.64  19448.69  19000.00  19070.56   \r\n",
      "2      2022-07-01 19:00:00  19070.57  19200.00  18975.00  19177.53   \r\n",
      "3      2022-07-01 19:30:00  19177.52  19226.68  19136.21  19191.03   \r\n",
      "4      2022-07-01 20:00:00  19191.02  19294.14  19130.21  19231.02   \r\n",
      "...                    ...       ...       ...       ...       ...   \r\n",
      "10493  2023-02-05 08:30:00  23314.05  23349.50  23230.00  23291.00   \r\n",
      "10494  2023-02-05 09:00:00  23290.99  23330.58  23261.22  23328.24   \r\n",
      "10495  2023-02-05 09:30:00  23329.46  23333.06  23290.66  23319.48   \r\n",
      "10496  2023-02-05 10:00:00  23319.86  23347.59  23312.40  23334.60   \r\n",
      "10497  2023-02-05 10:30:00  23335.19  23342.72  23292.00  23292.21   \r\n",
      "\r\n",
      "           Volume    return  rsi_rsi14    boll_upper  boll_middle  \\\r\n",
      "0      1610.29264  0.000113  24.738969  20521.539697   19750.2385   \r\n",
      "1      4395.40052 -0.018329  18.924991  20471.912982   19689.0125   \r\n",
      "2      2626.54183  0.005609  24.653652  20371.543428   19628.3890   \r\n",
      "3      1522.00827  0.000704  25.370346  20238.911453   19567.1175   \r\n",
      "4      1708.37449  0.002084  27.568222  20078.568585   19509.0680   \r\n",
      "...           ...       ...        ...           ...          ...   \r\n",
      "10493  5541.81519 -0.000989  31.192271  23489.382743   23405.6580   \r\n",
      "10494  3357.15964  0.001599  39.548807  23491.106841   23401.1420   \r\n",
      "10495  2426.03827 -0.000376  38.368373  23492.173346   23396.0435   \r\n",
      "10496  2863.65848  0.000648  41.607993  23489.713912   23391.3055   \r\n",
      "10497  2580.43314 -0.001817  35.909092  23492.575458   23385.5245   \r\n",
      "\r\n",
      "         boll_lower  \r\n",
      "0      18978.937303  \r\n",
      "1      18906.112018  \r\n",
      "2      18885.234572  \r\n",
      "3      18895.323547  \r\n",
      "4      18939.567415  \r\n",
      "...             ...  \r\n",
      "10493  23321.933257  \r\n",
      "10494  23311.177159  \r\n",
      "10495  23299.913654  \r\n",
      "10496  23292.897088  \r\n",
      "10497  23278.473542  \r\n",
      "\r\n",
      "[10498 rows x 11 columns]\r\n",
      "17\r\n",
      "state: 17, action: 1, action max value: 1.0, max step:10498\r\n"
     ]
    }
   ],
   "source": [
    "# 获得数据\r\n",
    "df = pd.read_csv('oac/BTC_train.csv')\r\n",
    "print(df)\r\n",
    "# 根据数据集设置环境\r\n",
    "env = StockTradingEnv(df)\r\n",
    "# T得到环境的参数信息（如：状态和动作的维度）\r\n",
    "state_dim = env.observation_space.shape[0]\r\n",
    "print(state_dim)\r\n",
    "action_dim = env.action_space.shape[0]\r\n",
    "\r\n",
    "max_action = float(env.action_space.high[0])\r\n",
    "max_step = len(df.loc[:, 'Open'].values)\r\n",
    "print(f'state: {state_dim}, action: {action_dim}, action max value: {max_action}, max step:{max_step}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "a5ee8ffb-2242-419c-83e9-912ccedfd72a",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-01-19T01:57:04.421632Z",
     "iopub.status.busy": "2024-01-19T01:57:04.421100Z",
     "iopub.status.idle": "2024-01-19T01:57:04.453374Z",
     "shell.execute_reply": "2024-01-19T01:57:04.452442Z",
     "shell.execute_reply.started": "2024-01-19T01:57:04.421601Z"
    },
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                     Time      Open      High       Low     Close      Volume  \\\r\n",
      "0     2023-02-05 11:00:00  23292.22  23339.00  23289.40  23313.82  3218.47246   \r\n",
      "1     2023-02-05 11:30:00  23313.82  23349.73  23301.34  23344.53  2933.81627   \r\n",
      "2     2023-02-05 12:00:00  23344.53  23345.60  23326.23  23338.42  2215.77951   \r\n",
      "3     2023-02-05 12:30:00  23338.95  23377.54  23337.48  23364.00  2821.43409   \r\n",
      "4     2023-02-05 13:00:00  23364.33  23390.00  23360.73  23369.61  2300.42999   \r\n",
      "...                   ...       ...       ...       ...       ...         ...   \r\n",
      "3495  2023-04-19 07:30:00  30332.79  30408.44  30317.01  30380.01   618.62109   \r\n",
      "3496  2023-04-19 08:00:00  30380.01  30413.53  30307.00  30314.36   822.31314   \r\n",
      "3497  2023-04-19 08:30:00  30314.35  30358.08  30282.95  30319.25   614.32757   \r\n",
      "3498  2023-04-19 09:00:00  30319.25  30348.28  30243.16  30310.08   732.41606   \r\n",
      "3499  2023-04-19 09:30:00  30310.09  30327.12  30223.12  30233.42   647.13137   \r\n",
      "\r\n",
      "        return  rsi_rsi14    boll_upper  boll_middle    boll_lower  \r\n",
      "0     0.000928  40.391377  23489.361531   23379.8210  23270.280469  \r\n",
      "1     0.001317  46.154551  23482.034946   23375.0660  23268.097054  \r\n",
      "2    -0.000262  45.217834  23473.880740   23370.0255  23266.170260  \r\n",
      "3     0.001096  49.810364  23463.832097   23365.9425  23268.052903  \r\n",
      "4     0.000240  50.784812  23455.433587   23362.6940  23269.954413  \r\n",
      "...        ...        ...           ...          ...           ...  \r\n",
      "3495  0.001557  61.302860  30445.233071   30250.4225  30055.611929  \r\n",
      "3496 -0.002161  56.817793  30451.081704   30257.0650  30063.048296  \r\n",
      "3497  0.000161  57.069741  30456.462710   30260.6370  30064.811290  \r\n",
      "3498 -0.000302  56.405127  30460.706019   30266.7815  30072.856981  \r\n",
      "3499 -0.002529  51.052526  30460.163708   30269.2095  30078.255292  \r\n",
      "\r\n",
      "[3500 rows x 11 columns]\r\n"
     ]
    }
   ],
   "source": [
    "# 获得数据\r\n",
    "eval_df = pd.read_csv('oac/BTC_test_pro.csv')[:3500]\r\n",
    "\r\n",
    "print(eval_df)\r\n",
    "# 根据数据集设置环境\r\n",
    "eval_env = StockTradingEnv(eval_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "cb24b04d-22c6-450d-ac22-5ca94d1d6a22",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-01-19T01:57:17.392653Z",
     "iopub.status.busy": "2024-01-19T01:57:17.392110Z",
     "iopub.status.idle": "2024-01-19T01:57:17.401460Z",
     "shell.execute_reply": "2024-01-19T01:57:17.400816Z",
     "shell.execute_reply.started": "2024-01-19T01:57:17.392620Z"
    },
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import parl\r\n",
    "import paddle\r\n",
    "import paddle.nn as nn\r\n",
    "import paddle.nn.functional as F\r\n",
    "\r\n",
    "class StockAgent(parl.Agent):\r\n",
    "    def __init__(self, algorithm, act_dim, expl_noise=0.1):\r\n",
    "        super(StockAgent, self).__init__(algorithm)\r\n",
    "\r\n",
    "        self.alg.sync_target(decay=0)\r\n",
    "        self.expl_noise = expl_noise\r\n",
    "        self.action_dim = act_dim\r\n",
    "\r\n",
    "    def predict(self, obs):\r\n",
    "        obs = paddle.to_tensor(obs.reshape(1, -1), dtype='float32')\r\n",
    "        action = self.alg.predict(obs)\r\n",
    "        action_numpy = action.cpu().numpy()[0]\r\n",
    "        return action_numpy\r\n",
    "\r\n",
    "    def sample(self, obs):\r\n",
    "        obs = paddle.to_tensor(obs.reshape(1, -1), dtype='float32')\r\n",
    "        action = self.alg.get_optimistic_exploration_action(obs)\r\n",
    "        action_numpy = action.cpu().numpy()[0]\r\n",
    "        return action_numpy\r\n",
    "\r\n",
    "    def learn(self, obs, action, reward, next_obs, terminal):\r\n",
    "        terminal = np.expand_dims(terminal, -1)\r\n",
    "        reward = np.expand_dims(reward, -1)\r\n",
    "\r\n",
    "        obs = paddle.to_tensor(obs, dtype='float32')\r\n",
    "        action = paddle.to_tensor(action, dtype='float32')\r\n",
    "        reward = paddle.to_tensor(reward, dtype='float32')\r\n",
    "        next_obs = paddle.to_tensor(next_obs, dtype='float32')\r\n",
    "        terminal = paddle.to_tensor(terminal, dtype='float32')\r\n",
    "        critic_loss, actor_loss = self.alg.learn(obs, action, reward, next_obs,\r\n",
    "                                                 terminal)\r\n",
    "        return critic_loss, actor_loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "71f7e5ae-bdb3-4a9d-b533-599e0896a2f7",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-01-19T01:57:19.815019Z",
     "iopub.status.busy": "2024-01-19T01:57:19.814429Z",
     "iopub.status.idle": "2024-01-19T01:57:22.870370Z",
     "shell.execute_reply": "2024-01-19T01:57:22.869583Z",
     "shell.execute_reply.started": "2024-01-19T01:57:19.814982Z"
    },
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "class ResidualBlock(nn.Layer):\r\n",
    "    def __init__(self, in_channels, out_channels):\r\n",
    "        super(ResidualBlock, self).__init__()\r\n",
    "\r\n",
    "        self.conv1 = nn.Conv1D(in_channels, out_channels, kernel_size=3, stride=1, padding=1)\r\n",
    "        self.bn1 = nn.BatchNorm1D(out_channels)\r\n",
    "        self.relu = nn.LeakyReLU()\r\n",
    "        self.conv2 = nn.Conv1D(out_channels, out_channels, kernel_size=3, stride=1, padding=1)\r\n",
    "        self.bn2 = nn.BatchNorm1D(out_channels)\r\n",
    "\r\n",
    "    def forward(self, x):\r\n",
    "        # x = paddle.unsqueeze(x, axis=-1) \r\n",
    "        residual = x\r\n",
    "        out = paddle.unsqueeze(x, axis=-1)  # transpose to shape [batch_size, seq_len, in_channels]\r\n",
    "        out = self.conv1(out)\r\n",
    "        out = self.bn1(out)\r\n",
    "        out = self.relu(out)\r\n",
    "        out = self.conv2(out)\r\n",
    "        out = self.bn2(out)\r\n",
    "        out += residual.unsqueeze(axis=-1) \r\n",
    "        out = self.relu(out)\r\n",
    "        out = paddle.squeeze(x, axis=-1)  # transpose back to original shape\r\n",
    "\r\n",
    "        return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "67a78599-ec99-411e-b5be-0986803bcdde",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-01-19T01:57:24.604199Z",
     "iopub.status.busy": "2024-01-19T01:57:24.603639Z",
     "iopub.status.idle": "2024-01-19T01:57:28.411296Z",
     "shell.execute_reply": "2024-01-19T01:57:28.410492Z",
     "shell.execute_reply.started": "2024-01-19T01:57:24.604162Z"
    },
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "class SelfAttention(parl.Model):\r\n",
    "    def __init__(self, hidden_dim, num_heads):\r\n",
    "        super(SelfAttention, self).__init__()\r\n",
    "        \r\n",
    "        self.hidden_dim = hidden_dim\r\n",
    "        self.num_heads = num_heads\r\n",
    "        \r\n",
    "        self.query_proj = nn.Linear(hidden_dim, hidden_dim)\r\n",
    "        self.key_proj = nn.Linear(hidden_dim, hidden_dim)\r\n",
    "        self.value_proj = nn.Linear(hidden_dim, hidden_dim)\r\n",
    "        self.attention = nn.MultiHeadAttention(hidden_dim, num_heads)\r\n",
    "        \r\n",
    "    def forward(self, x):\r\n",
    "        query = self.query_proj(x) # [256,1,20]\r\n",
    "        key = self.key_proj(x)\r\n",
    "        value = self.value_proj(x)\r\n",
    "\r\n",
    "        # transpose to match multihead attention input shape\r\n",
    "        query = query.transpose([1, 0, 2]) #[1,256,20]\r\n",
    "        key = key.transpose([1, 0, 2])\r\n",
    "        value = value.transpose([1, 0, 2])\r\n",
    "\r\n",
    "\r\n",
    "        # pass through multi-head attention layer\r\n",
    "        output = self.attention(query, key, value)\r\n",
    "\r\n",
    "        # transpose back to match original input shape\r\n",
    "        output = output.transpose([1, 0, 2])\r\n",
    "\r\n",
    "        return output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "5e0cb243-ab2f-44f0-a207-c4d84d887f1e",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-01-19T01:57:30.271048Z",
     "iopub.status.busy": "2024-01-19T01:57:30.270230Z",
     "iopub.status.idle": "2024-01-19T01:57:30.287815Z",
     "shell.execute_reply": "2024-01-19T01:57:30.287183Z",
     "shell.execute_reply.started": "2024-01-19T01:57:30.271009Z"
    },
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# clamp bounds for Std of action_log\r\n",
    "# action网络输出的标准差的上界和下界\r\n",
    "LOG_SIG_MAX = 2.0\r\n",
    "LOG_SIG_MIN = -20.0\r\n",
    "\r\n",
    "\r\n",
    "class StockModel(parl.Model):\r\n",
    "    def __init__(self, obs_dim, action_dim):\r\n",
    "        super(StockModel, self).__init__()\r\n",
    "        self.actor_model = Actor(obs_dim, action_dim)\r\n",
    "        self.critic_model = Critic(obs_dim, action_dim)\r\n",
    "\r\n",
    "    def policy(self, obs):\r\n",
    "        return self.actor_model(obs)\r\n",
    "\r\n",
    "    def value(self, obs, action):\r\n",
    "        return self.critic_model(obs, action)\r\n",
    "\r\n",
    "\r\n",
    "    def get_actor_params(self):\r\n",
    "        return self.actor_model.parameters()\r\n",
    "\r\n",
    "    def get_critic_params(self):\r\n",
    "        return self.critic_model.parameters()\r\n",
    "    \r\n",
    "\r\n",
    "\r\n",
    "class Actor(parl.Model):\r\n",
    "    def __init__(self, obs_dim, action_dim, hidden_dim=17, num_heads=17):\r\n",
    "        super(Actor, self).__init__()\r\n",
    "        self.self_attention = SelfAttention(hidden_dim, num_heads)\r\n",
    "        self.l1 = nn.Linear(obs_dim, 256)\r\n",
    "        self.res1 = ResidualBlock(256, 256)\r\n",
    "        self.l2 = nn.Linear(256, 256)\r\n",
    "        self.res2 = ResidualBlock(256, 256)\r\n",
    "        self.mean_linear = nn.Linear(256, action_dim)\r\n",
    "        self.std_linear = nn.Linear(256, action_dim)\r\n",
    "\r\n",
    "    def forward(self, obs):\r\n",
    "        obs = obs.unsqueeze(axis=-2)\r\n",
    "        x = self.self_attention(obs)\r\n",
    "\r\n",
    "        # 使用lstm提取特征\r\n",
    "        lstm = nn.LSTM(17, 256) \r\n",
    "        x,_= lstm(x)\r\n",
    "        x=x.squeeze(axis=-2)\r\n",
    "\r\n",
    "        x = F.relu(self.l1(obs))\r\n",
    "        x = x.squeeze(axis = -2)\r\n",
    "        x = self.res1(x)\r\n",
    "        x = F.relu(self.l2(x))\r\n",
    "        x = self.res2(x)\r\n",
    "\r\n",
    "\r\n",
    "        act_mean = paddle.tanh(self.mean_linear(x))\r\n",
    "        act_std = self.std_linear(x)\r\n",
    "        act_log_std = paddle.clip(act_std, min=LOG_SIG_MIN, max=LOG_SIG_MAX)\r\n",
    "        return act_mean, act_log_std\r\n",
    "\r\n",
    "\r\n",
    "class Critic(parl.Model):\r\n",
    "    def __init__(self, obs_dim, action_dim, hidden_dim=18, num_heads=3):\r\n",
    "        super(Critic, self).__init__()\r\n",
    "\r\n",
    "        self.self_attention = SelfAttention(hidden_dim, num_heads)\r\n",
    "        # Q1 network\r\n",
    "        self.l1 = nn.Linear(256, 256)\r\n",
    "        self.res1 = ResidualBlock(256, 256)\r\n",
    "        self.l2 = nn.Linear(256, 256)\r\n",
    "        self.res2 = ResidualBlock(256, 256)\r\n",
    "        self.l3 = nn.Linear(256, 1)\r\n",
    "\r\n",
    "        # Q2 network\r\n",
    "        self.l4 = nn.Linear(256, 256)\r\n",
    "        self.res3 = ResidualBlock(256, 256)\r\n",
    "        self.l5 = nn.Linear(256, 256)\r\n",
    "        self.res4 = ResidualBlock(256, 256)\r\n",
    "        self.l6 = nn.Linear(256, 1)\r\n",
    "\r\n",
    "    def forward(self, obs, action):\r\n",
    "        x = paddle.concat([obs, action], 1)\r\n",
    "        x = x.unsqueeze(axis=-2)\r\n",
    "        x = self.self_attention(x)\r\n",
    "        x = x.squeeze(axis=-2)\r\n",
    "\r\n",
    "        # 使用lstm提取特征\r\n",
    "        lstm = nn.LSTM(17 + 1 , 256) \r\n",
    "        x,_= lstm(x.unsqueeze(axis=-2))\r\n",
    "        x=x.squeeze(axis=-2)\r\n",
    "\r\n",
    "        # Q1\r\n",
    "        q1 = F.relu(self.l1(x))\r\n",
    "        # q1 = q1.squeeze(axis = -2)\r\n",
    "        q1 = self.res1(q1)\r\n",
    "        q1 = F.relu(self.l2(q1))\r\n",
    "        q1 = self.res2(q1)\r\n",
    "        q1 = self.l3(q1)\r\n",
    "\r\n",
    "        # Q2\r\n",
    "        q2 = F.relu(self.l4(x))\r\n",
    "        # q2 = q2.squeeze(axis = -2)\r\n",
    "        q2 = self.res3(q2)\r\n",
    "        q2 = F.relu(self.l5(q2))\r\n",
    "        q2 = self.res4(q2)\r\n",
    "        q2 = self.l6(q2)\r\n",
    "\r\n",
    "        return q1, q2\r\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "edcbf5c0-6fcf-49c6-b499-6a4b37d09777",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-01-19T01:57:34.273985Z",
     "iopub.status.busy": "2024-01-19T01:57:34.273355Z",
     "iopub.status.idle": "2024-01-19T01:57:34.279375Z",
     "shell.execute_reply": "2024-01-19T01:57:34.278611Z",
     "shell.execute_reply.started": "2024-01-19T01:57:34.273934Z"
    },
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "SEED = 678 # 随机种子\r\n",
    "WARMUP_STEPS = 10000\r\n",
    "EVAL_EPISODES = 3 # 评估的轮数\r\n",
    "MEMORY_SIZE = int(1e6)  # 经验池的大小\r\n",
    "BATCH_SIZE = 512  # 批次的大小\r\n",
    "GAMMA = 0.8180365520412749 # 折扣因子\r\n",
    "TAU = 0.0011142374179250148 # 当前网络参数比例，用于更新目标网络\r\n",
    "ACTOR_LR = 0.006732424266773069 # actor网络的参数\r\n",
    "CRITIC_LR = 0.0006900033317865891 # critic网络的参数\r\n",
    "alpha = 0.1406079080372761 # 熵正则化系数, SAC的参数\r\n",
    "beta = 4.66\r\n",
    "EXPL_NOISE = 0.1\r\n",
    "MAX_REWARD = -1e9 # 最大奖励\r\n",
    "file_name = f'OACmodel/OAC' # 模型保存的名字"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "1e8e5d0c-ec1d-43c0-a798-e2494b74e9c6",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-01-19T01:57:37.592160Z",
     "iopub.status.busy": "2024-01-19T01:57:37.591571Z",
     "iopub.status.idle": "2024-01-19T01:57:37.778650Z",
     "shell.execute_reply": "2024-01-19T01:57:37.777691Z",
     "shell.execute_reply.started": "2024-01-19T01:57:37.592130Z"
    },
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "model = StockModel(state_dim, action_dim)\r\n",
    "algorithm = OAC(\r\n",
    "        model,\r\n",
    "        gamma=GAMMA,\r\n",
    "        tau=TAU,\r\n",
    "        actor_lr=ACTOR_LR,\r\n",
    "        critic_lr=CRITIC_LR,\r\n",
    "        beta = beta,\r\n",
    "        delta = 23.53,\r\n",
    "        alpha = alpha)\r\n",
    "agent = StockAgent(algorithm, act_dim=action_dim, expl_noise=EXPL_NOISE)\r\n",
    "rpm = ReplayMemory(\r\n",
    "        max_size=MEMORY_SIZE, obs_dim=state_dim, act_dim=action_dim)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "a8423bb6-cf11-4ac8-b835-8bd5ddfad3f9",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-01-19T01:57:39.688012Z",
     "iopub.status.busy": "2024-01-19T01:57:39.687456Z",
     "iopub.status.idle": "2024-01-19T01:57:39.694930Z",
     "shell.execute_reply": "2024-01-19T01:57:39.694195Z",
     "shell.execute_reply.started": "2024-01-19T01:57:39.687978Z"
    },
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Runs policy for 5 episodes by default and returns average reward\r\n",
    "# A fixed seed is used for the eval environment\r\n",
    "eval_seed = [0, 53, 47, 99, 107, 1, 17, 57, 97, 179, 777]\r\n",
    "@paddle.no_grad()\r\n",
    "def run_evaluate_episodes(agent, env, eval_episodes):\r\n",
    "    avg_reward = 0.\r\n",
    "    for epi in range(eval_episodes):\r\n",
    "        obs = env.reset()\r\n",
    "        env.seed(eval_seed[epi])\r\n",
    "        done = False\r\n",
    "        while not done:\r\n",
    "            action = agent.predict(obs)\r\n",
    "            obs, reward, done, _ = env.step(action)\r\n",
    "            avg_reward += reward\r\n",
    "    avg_reward /= eval_episodes\r\n",
    "    print(f'Evaluator: the average reward is {avg_reward:.3f} over {eval_episodes} episodes.')\r\n",
    "    return avg_reward"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "23107eaa-69bc-460c-b1f7-82d65620ca71",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-01-19T01:57:41.261062Z",
     "iopub.status.busy": "2024-01-19T01:57:41.260393Z",
     "iopub.status.idle": "2024-01-19T01:57:41.268683Z",
     "shell.execute_reply": "2024-01-19T01:57:41.268071Z",
     "shell.execute_reply.started": "2024-01-19T01:57:41.261025Z"
    },
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Run episode for training\r\n",
    "import time\r\n",
    "def run_train_episode(agent, env, rpm,episode_num):\r\n",
    "    action_dim = env.action_space.shape[0]\r\n",
    "    obs = env.reset()\r\n",
    "    env.seed(SEED)\r\n",
    "    done = False\r\n",
    "    episode_reward = 0\r\n",
    "    episode_steps = 0\r\n",
    "    while not done:\r\n",
    "        episode_steps += 1\r\n",
    "        # Select action randomly or according to policy\r\n",
    "        # print(rpm.size())\r\n",
    "        if rpm.size() < WARMUP_STEPS:\r\n",
    "            action = np.random.uniform(-1, 1, size=action_dim)\r\n",
    "        else:\r\n",
    "            action = agent.sample(obs)\r\n",
    "        action = abs(action)\r\n",
    "        \r\n",
    "        next_obs, reward, done, info = env.step(action)\r\n",
    "        # if episode_steps%100 ==0:\r\n",
    "        #     print(action)\r\n",
    "        terminal = float(done)\r\n",
    "\r\n",
    "        # Store data in replay memory\r\n",
    "        rpm.append(obs, action, reward, next_obs, terminal)\r\n",
    "\r\n",
    "        obs = next_obs\r\n",
    "        episode_reward += reward\r\n",
    "\r\n",
    "        # Train agent after collecting sufficient data\r\n",
    "        if rpm.size() >= WARMUP_STEPS:\r\n",
    "            batch_obs, batch_action, batch_reward, batch_next_obs, batch_terminal = rpm.sample_batch(\r\n",
    "                BATCH_SIZE)\r\n",
    "            agent.learn(batch_obs, batch_action, batch_reward, batch_next_obs,\r\n",
    "                        batch_terminal)\r\n",
    "    # print(f'Learner: Episode {episode_steps+1} done. The reward is {episode_reward:.3f}.')\r\n",
    "    # 打印信息\r\n",
    "    current_step = info['current_step']\r\n",
    "    print(f'Learner: Episode {episode_num} done. The reward is {episode_reward:.3f}.')\r\n",
    "    print(info['status'])\r\n",
    "    return episode_reward, episode_steps"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "05733f9e-0f46-4278-a166-c0bfd4f889c9",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-01-19T01:57:43.616398Z",
     "iopub.status.busy": "2024-01-19T01:57:43.615782Z",
     "iopub.status.idle": "2024-01-19T01:57:57.084013Z",
     "shell.execute_reply": "2024-01-19T01:57:57.082729Z",
     "shell.execute_reply.started": "2024-01-19T01:57:43.616363Z"
    },
    "scrolled": true,
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/envs/python35-paddle120-env/lib/python3.7/site-packages/paddle/nn/layer/norm.py:654: UserWarning: When training, we now always track global mean and variance.\r\n",
      "  \"When training, we now always track global mean and variance.\")\r\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m/tmp/ipykernel_2641/753634994.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     22\u001b[0m         \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"运行时间：\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrunTime\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     23\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 24\u001b[0;31m \u001b[0mdo_train\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0magent\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0menv\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrpm\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m/tmp/ipykernel_2641/753634994.py\u001b[0m in \u001b[0;36mdo_train\u001b[0;34m(agent, env, rpm)\u001b[0m\n\u001b[1;32m     10\u001b[0m         \u001b[0;31m# Train episode\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     11\u001b[0m         \u001b[0mstart\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtime\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mperf_counter\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 12\u001b[0;31m         \u001b[0mepisode_reward\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mepisode_steps\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mrun_train_episode\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0magent\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0menv\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrpm\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mepisode_num\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     13\u001b[0m         \u001b[0mtotal_steps\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0mepisode_steps\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     14\u001b[0m         \u001b[0;32mif\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mepisode_num\u001b[0m\u001b[0;34m%\u001b[0m\u001b[0msave_freq\u001b[0m\u001b[0;34m==\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/tmp/ipykernel_2641/776788295.py\u001b[0m in \u001b[0;36mrun_train_episode\u001b[0;34m(agent, env, rpm, episode_num)\u001b[0m\n\u001b[1;32m     34\u001b[0m                 BATCH_SIZE)\n\u001b[1;32m     35\u001b[0m             agent.learn(batch_obs, batch_action, batch_reward, batch_next_obs,\n\u001b[0;32m---> 36\u001b[0;31m                         batch_terminal)\n\u001b[0m\u001b[1;32m     37\u001b[0m     \u001b[0;31m# print(f'Learner: Episode {episode_steps+1} done. The reward is {episode_reward:.3f}.')\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     38\u001b[0m     \u001b[0;31m# 打印信息\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/tmp/ipykernel_2641/3955088758.py\u001b[0m in \u001b[0;36mlearn\u001b[0;34m(self, obs, action, reward, next_obs, terminal)\u001b[0m\n\u001b[1;32m     34\u001b[0m         \u001b[0mterminal\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpaddle\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto_tensor\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mterminal\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'float32'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     35\u001b[0m         critic_loss, actor_loss = self.alg.learn(obs, action, reward, next_obs,\n\u001b[0;32m---> 36\u001b[0;31m                                                  terminal)\n\u001b[0m\u001b[1;32m     37\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mcritic_loss\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mactor_loss\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/OAC.py\u001b[0m in \u001b[0;36mlearn\u001b[0;34m(self, obs, action, reward, next_obs, terminal)\u001b[0m\n\u001b[1;32m    129\u001b[0m         critic_loss = self._critic_learn(obs, action, reward, next_obs,\n\u001b[1;32m    130\u001b[0m                                          terminal)\n\u001b[0;32m--> 131\u001b[0;31m         \u001b[0mactor_loss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_actor_learn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mobs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    132\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    133\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msync_target\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/OAC.py\u001b[0m in \u001b[0;36m_actor_learn\u001b[0;34m(self, obs)\u001b[0m\n\u001b[1;32m    153\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    154\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_actor_learn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mobs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 155\u001b[0;31m         \u001b[0mact\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlog_pi\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msample\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mobs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    156\u001b[0m         \u001b[0mq1_pi\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mq2_pi\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalue\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mobs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mact\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    157\u001b[0m         \u001b[0mmin_q_pi\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpaddle\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mminimum\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mq1_pi\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mq2_pi\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/OAC.py\u001b[0m in \u001b[0;36msample\u001b[0;34m(self, obs)\u001b[0m\n\u001b[1;32m     66\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     67\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0msample\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mobs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 68\u001b[0;31m         \u001b[0mact_mean\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mact_log_std\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpolicy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mobs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     69\u001b[0m         \u001b[0mnormal\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mNormal\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mact_mean\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mact_log_std\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mexp\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     70\u001b[0m         \u001b[0;31m# for reparameterization trick  (mean + std*N(0,1))\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/tmp/ipykernel_2641/131994928.py\u001b[0m in \u001b[0;36mpolicy\u001b[0;34m(self, obs)\u001b[0m\n\u001b[1;32m     12\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     13\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mpolicy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mobs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 14\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mactor_model\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mobs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     15\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     16\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mvalue\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mobs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maction\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/conda/envs/python35-paddle120-env/lib/python3.7/site-packages/paddle/fluid/dygraph/layers.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *inputs, **kwargs)\u001b[0m\n\u001b[1;32m    928\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    929\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 930\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_dygraph_call_func\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    931\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    932\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/conda/envs/python35-paddle120-env/lib/python3.7/site-packages/paddle/fluid/dygraph/layers.py\u001b[0m in \u001b[0;36m_dygraph_call_func\u001b[0;34m(self, *inputs, **kwargs)\u001b[0m\n\u001b[1;32m    913\u001b[0m                 \u001b[0moutputs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    914\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 915\u001b[0;31m             \u001b[0moutputs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    916\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    917\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mforward_post_hook\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_forward_post_hooks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/tmp/ipykernel_2641/131994928.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, obs)\u001b[0m\n\u001b[1;32m     48\u001b[0m         \u001b[0mx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mF\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrelu\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0ml1\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mobs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     49\u001b[0m         \u001b[0mx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msqueeze\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0maxis\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m-\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 50\u001b[0;31m         \u001b[0mx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mres1\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     51\u001b[0m         \u001b[0mx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mF\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrelu\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0ml2\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     52\u001b[0m         \u001b[0mx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mres2\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/conda/envs/python35-paddle120-env/lib/python3.7/site-packages/paddle/fluid/dygraph/layers.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *inputs, **kwargs)\u001b[0m\n\u001b[1;32m    928\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    929\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 930\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_dygraph_call_func\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    931\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    932\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/conda/envs/python35-paddle120-env/lib/python3.7/site-packages/paddle/fluid/dygraph/layers.py\u001b[0m in \u001b[0;36m_dygraph_call_func\u001b[0;34m(self, *inputs, **kwargs)\u001b[0m\n\u001b[1;32m    913\u001b[0m                 \u001b[0moutputs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    914\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 915\u001b[0;31m             \u001b[0moutputs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    916\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    917\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mforward_post_hook\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_forward_post_hooks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/tmp/ipykernel_2641/1799043511.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, x)\u001b[0m\n\u001b[1;32m     13\u001b[0m         \u001b[0mresidual\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     14\u001b[0m         \u001b[0mout\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpaddle\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0munsqueeze\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maxis\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# transpose to shape [batch_size, seq_len, in_channels]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 15\u001b[0;31m         \u001b[0mout\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mconv1\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mout\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     16\u001b[0m         \u001b[0mout\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbn1\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mout\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     17\u001b[0m         \u001b[0mout\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrelu\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mout\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/conda/envs/python35-paddle120-env/lib/python3.7/site-packages/paddle/fluid/dygraph/layers.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *inputs, **kwargs)\u001b[0m\n\u001b[1;32m    928\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    929\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 930\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_dygraph_call_func\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    931\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    932\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/conda/envs/python35-paddle120-env/lib/python3.7/site-packages/paddle/fluid/dygraph/layers.py\u001b[0m in \u001b[0;36m_dygraph_call_func\u001b[0;34m(self, *inputs, **kwargs)\u001b[0m\n\u001b[1;32m    913\u001b[0m                 \u001b[0moutputs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    914\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 915\u001b[0;31m             \u001b[0moutputs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    916\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    917\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mforward_post_hook\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_forward_post_hooks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/conda/envs/python35-paddle120-env/lib/python3.7/site-packages/paddle/nn/layer/conv.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, x)\u001b[0m\n\u001b[1;32m    345\u001b[0m             \u001b[0mdilation\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_dilation\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    346\u001b[0m             \u001b[0mgroups\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_groups\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 347\u001b[0;31m             data_format=self._data_format)\n\u001b[0m\u001b[1;32m    348\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mout\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    349\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/conda/envs/python35-paddle120-env/lib/python3.7/site-packages/paddle/nn/functional/conv.py\u001b[0m in \u001b[0;36mconv1d\u001b[0;34m(x, weight, bias, stride, padding, dilation, groups, data_format, name)\u001b[0m\n\u001b[1;32m    384\u001b[0m                  \u001b[0;34m'fuse_relu_before_depthwise_conv'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;32mFalse\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"padding_algorithm\"\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    385\u001b[0m                  padding_algorithm, \"data_format\", conv2d_data_format)\n\u001b[0;32m--> 386\u001b[0;31m         \u001b[0mout\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mgetattr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0m_C_ops\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0ml_type\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mweight\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0mattrs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    387\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mbias\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    388\u001b[0m             \u001b[0mout\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0melementwise_add\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mout\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbias\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maxis\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mchannel_dim\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "def do_train(agent, env, rpm):\r\n",
    "    save_freq = 1\r\n",
    "    total_steps = 0\r\n",
    "    num = 100\r\n",
    "    train_total_steps = len(df)*num\r\n",
    "    episode_num = 0\r\n",
    "    best_award = -1e9\r\n",
    "    while total_steps < train_total_steps:\r\n",
    "        episode_num +=1\r\n",
    "        # Train episode\r\n",
    "        start = time.perf_counter()\r\n",
    "        episode_reward, episode_steps = run_train_episode(agent, env, rpm,episode_num)\r\n",
    "        total_steps += episode_steps\r\n",
    "        if(episode_num%save_freq==0):\r\n",
    "            avg_reward = run_evaluate_episodes(agent, eval_env, EVAL_EPISODES)\r\n",
    "            if(best_award<avg_reward):\r\n",
    "                best_award = avg_reward\r\n",
    "                print(f'Saving best model!')\r\n",
    "                agent.save(f\"./models/{file_name}.ckpt\")\r\n",
    "        end = time.perf_counter()\r\n",
    "        runTime = end - start\r\n",
    "        print(\"运行时间：\", runTime)\r\n",
    "\r\n",
    "do_train(agent, env, rpm)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "93ea54ef-36e3-4acd-9440-752b556590e0",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-01-19T01:44:09.894418Z",
     "iopub.status.busy": "2024-01-19T01:44:09.893786Z",
     "iopub.status.idle": "2024-01-19T01:44:09.902427Z",
     "shell.execute_reply": "2024-01-19T01:44:09.901770Z",
     "shell.execute_reply.started": "2024-01-19T01:44:09.894383Z"
    },
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "def run_test_episodes(agent, env, eval_episodes,max_action_step = 200):\r\n",
    "    avg_reward = 0.\r\n",
    "    avg_worth = 0.\r\n",
    "    for _ in range(eval_episodes):\r\n",
    "        obs = env.reset()\r\n",
    "        env.seed(1)\r\n",
    "        done = False\r\n",
    "        t = 0\r\n",
    "        Buy = []\r\n",
    "        Sell = []\r\n",
    "        while not done:\r\n",
    "            action = agent.predict(obs)\r\n",
    "            # action = (action+1.0)/2.0\r\n",
    "            action = abs(action)\r\n",
    "            obs, reward, done, info = env.step(action)\r\n",
    "            avg_reward += reward\r\n",
    "            t+=1\r\n",
    "            if(t==max_action_step):\r\n",
    "                # eval_env.render()\r\n",
    "                print('over')\r\n",
    "                break\r\n",
    "        avg_worth += info['profit']\r\n",
    "        # eval_env.render()\r\n",
    "        Buy=info['buy']\r\n",
    "        Sell=info[\"sell\"]\r\n",
    "        Buy_v=info['buy_v']\r\n",
    "        Sell_v=info['sell_v']\r\n",
    "        get = info['get']\r\n",
    "    avg_reward /= eval_episodes\r\n",
    "    avg_worth /= eval_episodes\r\n",
    "    print(f'Evaluator: The average reward is {avg_reward:.3f} over {eval_episodes} episodes.')\r\n",
    "    print(f'Evaluator: The average worth is {avg_worth:.3f} over {eval_episodes} episodes.')\r\n",
    "\r\n",
    "    return avg_reward,Buy,Sell,Buy_v,Sell_v,get"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "26d264f6-a067-4aa7-b456-13eacf83cb13",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# 获得数据\r\n",
    "test_df = pd.read_csv('oac/BTC_test_pro.csv')\r\n",
    "# 根据数据集设置环境\r\n",
    "env = StockTradingEnv(test_df)\r\n",
    "agent.restore('models/OACmodel/OAC.ckpt')\r\n",
    "# 设置的最大执行的天数，每一个step表示一天\r\n",
    "max_action_step = len(test_df)\r\n",
    "avg_reward,Buy,Sell,Buy_v,Sell_v,get = run_test_episodes(agent, env, EVAL_EPISODES,max_action_step)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "py35-paddle1.2.0"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
